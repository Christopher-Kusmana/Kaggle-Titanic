{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e427818-3429-4844-a4f3-391aabd3cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3eee62-f89d-4512-9321-a88e1c4f3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1765e6-be78-4029-b322-3406201ffa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 data samples for reference\n",
    "main_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ec4e4-f8b9-4367-88d9-1b5d3bb0dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff10f36-b3cc-4ecb-9888-dfce49bf2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.shape\n",
    "# Main dataframe has 891 training data with 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55c0cc-dd8a-404f-8d99-ca2fcd819022",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.info()\n",
    "# Most features are numerical except for ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df6463-7a4d-44d1-9a5e-81b599d6169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.isna().sum()\n",
    "\n",
    "# There a 2 missing values in the feature 'Embarked', \n",
    "# about 20% of the Ages are missing, and \n",
    "# the majority of the Cabins are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52052f-838a-41db-a606-1ed9abb49a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with missing values (Embarked)\n",
    "\n",
    "main_df['Embarked'].value_counts()\n",
    "# Since there are only 2 missing values for Embarked, filling the missing value with the mode \n",
    "# of the dataset is reasonable. In this case we fill it with 'S' which takes account about 70% of 'Embarked'\\\n",
    "main_df['Embarked'].fillna('S', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23abd4-1317-45bc-936e-e06f25c4b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with missing values (Age)\n",
    "# To fill the missing age value as accurately as possible, I would use the passanger's title/salutation as a \n",
    "# benchmark. I first create a new column by applying a lambda function that extracts the passanger's title, \n",
    "# taking advantage of the consistent string structure\n",
    "main_df['Salutation'] = main_df.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "main_df_subage = main_df.groupby(['Salutation'])\n",
    "# We created a sub data frame that groups the main dataframe by the passanger's salutation, we then describe \n",
    "# the dataframe, giving us insights to how to fill in the corresponding missing values\n",
    "main_df_subage.describe()\n",
    "main_df_subage.hist('Age', by='Salutation')\n",
    "\n",
    "# Since not all ages are normally distributed according to their titles, we would fill the missing value with\n",
    "# the median age of each title\n",
    "\n",
    "# Filling missing values\n",
    "\n",
    "# Calculate median age for each salutation\n",
    "median_age_by_salutation = main_df_subage['Age'].median()\n",
    "\n",
    "# Define a function to fill missing age values based on salutation\n",
    "def fill_missing_age(row):\n",
    "    if pd.isnull(row['Age']):\n",
    "        return median_age_by_salutation[row['Salutation']]\n",
    "    else:\n",
    "        return row['Age']\n",
    "\n",
    "# Apply the function to fill missing age values\n",
    "main_df['Age'] = main_df.apply(fill_missing_age, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebe53a-f724-4bb0-8ec8-b10af794975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with missing values (Cabin)\n",
    "main_df['Cabin'].unique()\n",
    "main_df['Cabin'].value_counts()\n",
    "\n",
    "# After analyzing the value counts and unique cabin values, it is very difficult to come up with a way to methodically\n",
    "# fill the missing values without sacrificing accuracy. Since cabin numbers are unique (except for family who might \n",
    "# be in the same cabin), it is not reasonable to fill the missing value with the mode or most common cabin. \n",
    "# With this in mind, we will make the missing cabins as 'Unknown'\n",
    "\n",
    "main_df['Cabin'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71760cc8-bc71-4206-8922-71a52e51ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that missing value has been handled, let's move on to feature engineering\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# First thing is to drop the 'Name' column because it is highly irrelevant to survival \n",
    "# +main_df.drop(columns=['Name'], inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502ef58-371a-459d-a02e-57399bd8154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# We would keep 'Survived' for it is the target variable \n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6941b2-9f72-4ca9-b665-4d3482a64bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Next, let us analyze the feature Pclass \n",
    "\n",
    "# Create a pivot table that calculates survival mean of each passanger depending on their 'Pclass'\n",
    "pivot_table = main_df.pivot_table(index='Pclass', values='Survived')\n",
    "sns.heatmap(pivot_table, cmap='viridis', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Finds the correlation matrix between Pclass and Survived\n",
    "correlation_matrix = main_df[['Pclass'] + ['Survived']].corr()\n",
    "\n",
    "# Plot heatmap using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# INSIGHT: There seemed to be a downward trend of 'Survived' as 'Pclass' goes up. Additionally,\n",
    "# correlation matrix shows a moderate negative linear correlation betweeen 'Pclass' and our target \n",
    "# variable, 'Survived'. This means that this feature is relevant and will be kept for our model\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a61f84-d45c-4a0c-91a7-abec7c62773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Next, let us analyze the feature 'Sex'\n",
    "# First we need to apply Label Encoder to convert this categorical column into a numerical one\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode 'Sex' column\n",
    "\n",
    "main_df['Sex_Encoded'] = label_encoder.fit_transform(main_df['Sex'])\n",
    "# Encoded male = 1, Encoded female = 0\n",
    "\n",
    "# Create a pivot table that calculates survival mean of each passanger depending on their 'Sex'\n",
    "pivot_table = main_df.pivot_table(index='Sex_Encoded', values='Survived')\n",
    "sns.heatmap(pivot_table, cmap='viridis', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Finds the correlation matrix between Sex and Survived\n",
    "correlation_matrix = main_df[['Sex_Encoded'] + ['Survived']].corr()\n",
    "\n",
    "# Plot heatmap using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# INSIGHT: In this feature, we found a strong negative correlation between sex and survival. Moreover,\n",
    "# we also see a heavy dip in survival from female (0) to male (1). Therefore, 'Sex' is indeed an important\n",
    "# feature for our model\n",
    "\n",
    "# --------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1afe1ca-9c2f-495d-8e6a-a8b9ac0fcd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Next, let us analyze age\n",
    "# We need to group ages into bins since it is not practical to construct a pivot table for EVERY single\n",
    "# age in the dataframe. Let us divide our age group into 8 bins and plot the corresponding heatmap\n",
    "\n",
    "age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\n",
    "main_df['AgeGroup'] = pd.cut(main_df['Age'], bins=age_bins)\n",
    "pivot_table = main_df.pivot_table(index='AgeGroup', values='Survived')\n",
    "sns.heatmap(pivot_table, cmap='viridis', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Next, we can plot the correlation heatmap between 'Age' and output variable \n",
    "\n",
    "# Finds the correlation matrix between Age and Survived\n",
    "correlation_matrix = main_df[['Age'] + ['Survived']].corr()\n",
    "\n",
    "# Plot heatmap using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# INSIGHT: From these two heatmaps, we actually got two conflicting result. The first heatmap from the pivot table shows us that \n",
    "# younger people tend to survive, hence the left skew. The correlation matrix however, states that there is almost no linear corelation at all between\n",
    "# the two variables. To further confirm this, I will investigate by creating a sub-feature called AgeGroup, to further highlight the decrease trend\n",
    "# in survival as age increases \n",
    "\n",
    "# Define age groups\n",
    "age_bins = [0, 4, 13, 19, 24, 65, main_df['Age'].max()]\n",
    "age_labels = ['Infant', 'Child', 'Teen', 'Young Adult', 'Adult', 'Elderly']\n",
    "age_group_mapping = {'Infant': 0, 'Child': 1, 'Teen': 2, 'Young Adult': 3, 'Adult': 4, 'Elderly': 5}\n",
    "\n",
    "# Create 'AgeGroup' column based on age groups (binning)\n",
    "main_df['AgeGroup'] = pd.cut(main_df['Age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Calculate survival rate by age group\n",
    "survival_rate_by_age_group = main_df.groupby('AgeGroup')['Survived'].mean()\n",
    "print(survival_rate_by_age_group)\n",
    "\n",
    "# INSIGHT: We indeed do see a negative correlation between age groups with younger passangers having a higher survival chance. It also seemed like the \n",
    "# decrease is not linear which caused the correlation matrix to be a weak linear negative correlation. However, by analyzing the trend, age indeed is a\n",
    "# relevant feature for predicting survival rate\n",
    "\n",
    "# --------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ee1c6-fcea-42c2-96a2-f0bd0c697e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Next we will analyze the feature SibSp (# sib/spouses) and Parch (# of parents)\n",
    "\n",
    "# Pivot table that reveals the mean of Survived in each category of Parch and SibSp\n",
    "pivot_table = main_df.pivot_table(index='Parch', values='Survived')\n",
    "sns.heatmap(pivot_table, cmap='viridis', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "pivot_table = main_df.pivot_table(index='SibSp', values='Survived')\n",
    "sns.heatmap(pivot_table, cmap='viridis', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# INSIGHT: After observing the mean survival for both Parch and SibSp, there seemed to be no trend in\n",
    "# how the two features moves with 'Survived'. To further confirm this, let us plot the correlation matrix\n",
    "# between the two features against 'Survived'\n",
    "\n",
    "\n",
    "# Finds the correlation matrix between Parch and Survived\n",
    "correlation_matrix = main_df[['Parch'] + ['Survived']].corr()\n",
    "\n",
    "# Plot heatmap using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Finds the correlation matrix between SibSp and Survived\n",
    "correlation_matrix = main_df[['SibSp'] + ['Survived']].corr()\n",
    "\n",
    "# Plot heatmap using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# As hypothesized, there is no linear correlation between having more sibling/parents/spouses/children on board\n",
    "# with survival, meaning that the variation in survival rate between groups happened by chance. However, I do want\n",
    "# to further explore why the higher SibSp and Parch group has 0 survival chance\n",
    "\n",
    "main_df[(main_df['Parch'] > 3) | (main_df['SibSp'] > 4)]\n",
    "\n",
    "# This confirms our hypothesis that such data group are considered as outliers since there are only select numbers\n",
    "# of passanger who has high SibSp or Parch. Another thing to note is that all of them has Pclass value of 3, meaning\n",
    "# that Pclass more than likely played a bigger role than SibSp and Parch\n",
    "\n",
    "# INSIGHT: If we discard the outliers (high value SibSp and Parch), we would get a pretty much 50% survival rate\n",
    "# from both SibSp and Parch across all groups meaning that it is probably safe to discard these features from our model\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f322d-8a41-4568-9adf-91d5d0c25b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Next, let us take a deeper dive into the Ticket and Fare feature\n",
    "\n",
    "# I want to explore whether or not higher fare increases the survival rate of passangers\n",
    "main_df['Fare'].describe()\n",
    "main_df.loc[(main_df['Fare'] > main_df['Fare'].mean()), 'Survived'].value_counts(normalize=True)\n",
    "\n",
    "# INSIGHT: Sure enough, the higher the fare the higher the survival chance. Though not exactly linear, the two \n",
    "# variables definitely move together.\n",
    "\n",
    "# Next is the trickier part, we want to see how having different Tickets might affect your survival in the Titanic\n",
    "# My first thought is that more expensive ticket might be marked with longer length or having a prefix. This issue might \n",
    "# cause multicolinearity. To confirm this, I did some data manipulation and created a pivot table that illustrate\n",
    "# the average fare for each ticket length\n",
    "\n",
    "sub_df = main_df\n",
    "sub_df['TicketLen'] = main_df['Ticket'].apply(len)\n",
    "# Finds the correlation matrix between SibSp and Survived\n",
    "pivot_table = sub_df.pivot_table(index='TicketLen', values='Fare')\n",
    "sns.heatmap(pivot_table, cmap='viridis', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# There is a spike in price mean at Tickenlen == 8, I would like to investigae it further whether it is\n",
    "# the case that ticket with length 8 is more expensive or this is simply an outlier\n",
    "\n",
    "sub_df.loc[(sub_df['TicketLen'] == 8), ['Fare']].hist()\n",
    "\n",
    "# Sure enough, this case seemed to be an outlier as visualized by the histogram above\n",
    "# To further confirm this, let's see if there is a correlation between TicketLen and Fare using heatmap correlation matrix\n",
    "\n",
    "# Finds the correlation matrix between TicketLen and Fare\n",
    "correlation_matrix = sub_df[['TicketLen'] + ['Fare']].corr()\n",
    "\n",
    "# Plot heatmap using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# INSIGHT: There is no correlation between TicketLen and Fare, which means that we still need to further engineer\n",
    "# the feature Ticket and extract what attribute of the ticket might be correlated to survival (maybe the presence of letter,\n",
    "# certain number prefix, combination, etc). But for this model, we can simply drop Ticket feature while keeping Fare\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32d40d-9afc-42a1-ae23-a0abfe0ea717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Now we wil explore the Cabin feature\n",
    "main_df['Cabin'].unique()\n",
    "\n",
    "# There are too many unique cabin values to work with, let us create a new feature that only takes the first \n",
    "# letter (or the class) of the cabin called 'CabinLetter'.\n",
    "\n",
    "main_df['CabinLetter'] = main_df['Cabin'].str[0]\n",
    "\n",
    "# Constructs a pivot table to see how the survival rate is distributed across different cabin sections\n",
    "pivot_table = sub_df.pivot_table(index='CabinLetter', values='Survived')\n",
    "sns.heatmap(pivot_table, cmap='viridis', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# INSIGHT: Passangers from cabin B, D, E, and F have relatively high survival chance compared to the others\n",
    "main_df['Cabin_Encoded'] = label_encoder.fit_transform(main_df['CabinLetter'])\n",
    "\n",
    "# Let us encode the CabinLetter so we can convert the categorical value to \n",
    "# a numerical one to visualize the correlation between CabinLetter to Survived \n",
    "\n",
    "# Calculate the correlation matrix between Cabin_Encoded and Survived\n",
    "correlation_matrix = sub_df[['Cabin_Encoded'] + ['Survived']].corr()\n",
    "\n",
    "# Plot heatmap using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# There is a moderate negative linear correlation between CabinLetter and Survived, meaning that we will take Cabin_Encoded \n",
    "# as our selected feature\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0661a-1817-4b21-bbb8-a9e9bf873403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Finally, let us analyze the feature Embarked\n",
    "\n",
    "# Let's apply the same technique as we did with Cabin, simply construct a pivot table to see the overall \n",
    "# distribution first\n",
    "\n",
    "# Constructs a pivot table to see how the survival rate is distributed across different cabin sections\n",
    "pivot_table = sub_df.pivot_table(index='Embarked', values='Survived')\n",
    "sns.heatmap(pivot_table, cmap='viridis', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# INSIGHT: It seemed like passangers who are embarked from C, O, and S has a descending survival rate respectively\n",
    "# Though the trend appears to be nonlinear, let's encode these values and consutrct a correlation matrix to further confirm this.\n",
    "\n",
    "main_df['Embarked_Encoded'] = label_encoder.fit_transform(main_df['Embarked'])\n",
    "\n",
    "# Calculate the correlation matrix between Embraked_Encoded and Survived\n",
    "correlation_matrix = sub_df[['Embarked_Encoded'] + ['Survived']].corr()\n",
    "\n",
    "# Plot heatmap using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# There seemed to be a somewhat negative linear corrleation between survival and from where the passanger\n",
    "# embarked. For this version of the model, let's include this as our feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49005bc-f982-46d7-b60b-781ab960a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin model training\n",
    "\n",
    "training_x = main_df[['Pclass', 'Sex_Encoded', 'Age', 'Fare', 'Cabin_Encoded', 'Embarked_Encoded']]\n",
    "training_y = main_df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_x, training_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models to evaluate\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    SVC(),\n",
    "    GaussianNB(),\n",
    "    XGBClassifier()\n",
    "]\n",
    "\n",
    "# Evaluate models using cross-validation on training data\n",
    "best_model = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "for model in models:\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    mean_score = np.mean(scores)\n",
    "\n",
    "    if mean_score > best_score:\n",
    "        best_model = model\n",
    "        best_score = mean_score\n",
    "\n",
    "# Train the best model on the full training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_score = accuracy_score(y_test, best_model.predict(X_test))\n",
    "\n",
    "print(f\"Best Model: {type(best_model).__name__}\")\n",
    "print(f\"Cross-validation Mean Accuracy: {best_score:.4f}\")\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")\n",
    "\n",
    "# INSIGHT: Gradient Boosting Classifier model gives the highest predictive accuracy of ~0.81\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef65ce-ae29-4234-aff7-fb0cc22249a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will apply the model on the testing data and convert our result to csv to submit\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314d0e0-7ec0-49b5-9ef1-7d21b8261d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same data processing methods\n",
    "# Fill missing age\n",
    "\n",
    "test_df.isna().sum()\n",
    "\n",
    "test_df['Salutation'] = test_df.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "test_subage = test_df.groupby(['Salutation'])\n",
    "\n",
    "\n",
    "median_age_by_salutation = test_subage['Age'].median()\n",
    "def fill_missing_age(row):\n",
    "    if pd.isnull(row['Age']):\n",
    "        return median_age_by_salutation[row['Salutation']]\n",
    "    else:\n",
    "        return row['Age']\n",
    "test_df['Age'] = test_df.apply(fill_missing_age, axis=1)\n",
    "\n",
    "# The only Ms in the dataset has a missing value, so we will use the median from the training set\n",
    "\n",
    "test_df.loc[88, ['Age']] = [21]\n",
    "\n",
    "\n",
    "# Fill missing Cabin\n",
    "\n",
    "test_df['Cabin'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Fill missing fare with median \n",
    "\n",
    "test_df['Fare'].fillna(test_df['Fare'].median(), inplace=True)\n",
    "\n",
    "# Encode Sex, CabinLetter, and Embarked\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "test_df['Sex_Encoded'] = label_encoder.fit_transform(test_df['Sex'])\\\n",
    "\n",
    "test_df['CabinLetter'] = test_df['Cabin'].str[0]\n",
    "test_df['Cabin_Encoded'] = label_encoder.fit_transform(test_df['CabinLetter'])\n",
    "\n",
    "test_df['Embarked_Encoded'] = label_encoder.fit_transform(test_df['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d09f2e-c802-44e1-9fcb-e783b7d9ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_test = test_df[['Pclass', 'Sex_Encoded', 'Age', 'Fare', 'Cabin_Encoded', 'Embarked_Encoded']]\n",
    "y_res = best_model.predict(x_pred_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\" : test_df['PassengerId'],\n",
    "    \"Survived\" : y_res\n",
    "})\n",
    "\n",
    "submission.to_csv(\"titanicsubmissionrf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549c266-34cb-43c4-a6f6-72f371c9568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model subset features = ['Pclass', 'Sex_Encoded', 'Age', 'Fare', 'Cabin_Encoded', 'Embarked_Encoded']\n",
    "# Best model: Gradient Boosting Classifier\n",
    "# Data processing and Cleaning:\n",
    "# - Missing ages replaced with the median of the passenger's corresponding salutation\n",
    "# - Missing Cabin replaced to Unknown\n",
    "# - Missing embarked replaced with the mode of the dataset\n",
    "# - Missing Fare replaced with the median of the dataset\n",
    "# - Sex encoded with label encoder\n",
    "# - CabinLetter column created by taking the first string of the cabin then encoded with label encoder\n",
    "# - Embarked encoded with label encoder\n",
    "\n",
    "# The final submission results in ~0.8 accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
